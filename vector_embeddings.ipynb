{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d98aaf-d00d-4350-adcb-597213bdf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1dd0d7-c4b9-4dc7-bdf2-c8d1ca1db6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1d0db-8891-4c08-a73b-6ea52578eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9588fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_2Y7bDH_4oYyB2hHTaMzvyQaYi1kZYL8tVaDMAGbdPWFJwvsZs7vWrL5stJRCmkTEZCb9kR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"developer-quickstart-py\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            \"field_map\":{\"text\": \"chunk_text\"}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "import pinecone\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load API keys\n",
    "# -------------------------\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"API_KEY_OPENAI\")       # üîë Match with .env\n",
    "pinecone_key = os.getenv(\"API_KEY_PINECONE\")   # üîë Match with .env\n",
    "\n",
    "if not openai_key or not pinecone_key:\n",
    "    raise ValueError(\"‚ùå Missing API keys. Check your .env file.\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Extract text from PDF\n",
    "# -------------------------\n",
    "pdf_path = r\"E:\\Plant_ML\\AI\\Roger S. Pressman_ Bruce R. Maxin - Software Engineering_ A Practitioner‚Äôs Approach-McGraw-Hill Edu.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "full_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        full_text += text + \"\\n\"\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split text into chunks\n",
    "# -------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Initialize clients\n",
    "# -------------------------\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "# ‚úÖ Correct Pinecone init\n",
    "pc = pinecone.Pinecone(api_key=pinecone_key)\n",
    "index_name = \"developer-quickstart-py\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Embed and store chunks\n",
    "# -------------------------\n",
    "for i, chunk in enumerate(chunks):\n",
    "    embedding_response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",  # ‚úÖ use OpenAI‚Äôs embedding model\n",
    "        input=chunk\n",
    "    )\n",
    "    vector = embedding_response.data[0].embedding\n",
    "\n",
    "    index.upsert([\n",
    "        {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"values\": vector,\n",
    "            \"metadata\": {\"chunk_text\": chunk, \"page\": i}\n",
    "        }\n",
    "    ])\n",
    "\n",
    "print(f\"‚úÖ Stored {len(chunks)} chunks from PDF into Pinecone\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Query the index\n",
    "# -------------------------\n",
    "query_text = \"Explain the Waterfall software process model\"\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=query_text\n",
    ").data[0].embedding\n",
    "\n",
    "results = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(\"\\nüîé Top Retrieved Chunks:\")\n",
    "for match in results[\"matches\"]:\n",
    "    print(f\"Score: {match['score']:.4f}\")\n",
    "    print(f\"Page: {match['metadata']['page']}\")\n",
    "    print(f\"Text: {match['metadata']['chunk_text'][:300]}...\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. Generate final answer\n",
    "# -------------------------\n",
    "context = \"\\n\\n\".join([m[\"metadata\"][\"chunk_text\"] for m in results[\"matches\"]])\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert tutor in software engineering.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Based on the following context, answer the question:\\n\\n{context}\\n\\nQuestion: {query_text}\"}\n",
    "    ]a\n",
    ")\n",
    "\n",
    "print(\"\\nüìù Final Answer:\")\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv pinecone-client pypdf langchain google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"pcsk_2Y7bDH_4oYyB2hHTaMzvyQaYi1kZYL8tVaDMAGbdPWFJwvsZs7vWrL5stJRCmkTEZCb9kR\")\n",
    "\n",
    "index_name = \"developer-quickstart-py\"\n",
    "\n",
    "# If index already exists with wrong dimension, delete it first\n",
    "if pc.has_index(index_name):\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# Create new index with correct embedding model dimension\n",
    "pc.create_index_for_model(\n",
    "    name=index_name,\n",
    "    cloud=\"aws\",\n",
    "    region=\"us-east-1\",\n",
    "    embed={\n",
    "        \"model\": \"llama-text-embed-v2\",   # this will auto-set dimension to 1024\n",
    "        \"field_map\": {\"text\": \"chunk_text\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "print(f\"Index {index_name} is ready with correct dimensions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8269805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_2Y7bDH_4oYyB2hHTaMzvyQaYi1kZYL8tVaDMAGbdPWFJwvsZs7vWrL5stJRCmkTEZCb9kR\")\n",
    "\n",
    "# delete old index\n",
    "pc.delete_index(\"developer-quickstart-py\")\n",
    "\n",
    "# create new index with 768 dimension\n",
    "pc.create_index(\n",
    "    name=\"developer-quickstart-py\",\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load Pinecone key\n",
    "load_dotenv()\n",
    "pinecone_key = os.getenv(\"API_KEY_PINECONE\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = pinecone.Pinecone(api_key=pinecone_key)\n",
    "index_name = \"developer-quickstart-py\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Delete all vectors in the index\n",
    "index.delete(delete_all=True)\n",
    "\n",
    "print(\"‚úÖ All vectors cleared from Pinecone index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8975e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import pinecone\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load API keys\n",
    "# -------------------------\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv(\"API_KEY_GEMINI\")       # üîë Gemini key in .env\n",
    "pinecone_key = os.getenv(\"API_KEY_PINECONE\")   # üîë Pinecone key in .env\n",
    "\n",
    "if not gemini_key or not pinecone_key:\n",
    "    raise ValueError(\"‚ùå Missing API keys. Check your .env file.\")\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=gemini_key)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Extract text from PDF\n",
    "# -------------------------\n",
    "pdf_path = r\"E:\\Plant_ML\\AI\\Roger S. Pressman_ Bruce R. Maxin - Software Engineering_ A Practitioner‚Äôs Approach-McGraw-Hill Edu.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "full_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        full_text += text + \"\\n\"\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split text into chunks\n",
    "# -------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Initialize Pinecone\n",
    "# -------------------------\n",
    "pc = pinecone.Pinecone(api_key=pinecone_key)\n",
    "index_name = \"developer-quickstart-py\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Embed and store chunks\n",
    "# -------------------------\n",
    "for i, chunk in enumerate(chunks):\n",
    "    embedding_response = genai.embed_content(\n",
    "        model=\"models/embedding-001\",  # ‚úÖ Gemini embedding model\n",
    "        content=chunk\n",
    "    )\n",
    "    vector = embedding_response[\"embedding\"]\n",
    "\n",
    "    index.upsert([\n",
    "        {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"values\": vector,\n",
    "            \"metadata\": {\"chunk_text\": chunk, \"page\": i}\n",
    "        }\n",
    "    ])\n",
    "\n",
    "print(f\"‚úÖ Stored {len(chunks)} chunks from PDF into Pinecone\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Query the index\n",
    "# -------------------------\n",
    "query_text = \"Explain the Waterfall software process model\"\n",
    "\n",
    "query_embedding = genai.embed_content(\n",
    "    model=\"models/embedding-001\",\n",
    "    content=query_text\n",
    ")[\"embedding\"]\n",
    "\n",
    "results = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(\"\\nüîé Top Retrieved Chunks:\")\n",
    "for match in results[\"matches\"]:\n",
    "    print(f\"Score: {match['score']:.4f}\")\n",
    "    print(f\"Page: {match['metadata']['page']}\")\n",
    "    print(f\"Text: {match['metadata']['chunk_text'][:300]}...\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. Generate final answer\n",
    "# -------------------------\n",
    "context = \"\\n\\n\".join([m[\"metadata\"][\"chunk_text\"] for m in results[\"matches\"]])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\n",
    "    f\"Based on the following context, answer the question:\\n\\n{context}\\n\\nQuestion: {query_text}\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìù Final Answer:\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# -------------------------\n",
    "# PDF to chunks\n",
    "# -------------------------\n",
    "pdf_path = r\"E:\\Plant_ML\\AI\\Roger S. Pressman_ Bruce R. Maxin - Software Engineering_ A Practitioner‚Äôs Approach-McGraw-Hill Edu.pdf\"\n",
    "output_folder = r\"E:\\Plant_ML\\AI\\pdf_chunks\"   # folder to save chunks\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "full_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        full_text += text + \"\\n\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "# Save chunks as JSON (one file) or individual text files\n",
    "for i, chunk in enumerate(chunks):\n",
    "    with open(os.path.join(output_folder, f\"chunk_{i}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(chunk)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(chunks)} chunks into {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_2t6mSD_RayKmzruxJCHqLK3yYSMkGVU7tSitdiF2dvyob3vVYkTcpMVreLQRmsxerCkpRv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"acadmate\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            \"field_map\":{\"text\": \"chunk_text\"}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone index for Gemini embeddings\n",
    "pc.create_index(\n",
    "    name=\"acadmate-gemini\",\n",
    "    dimension=768,          # Gemini embedding dimension\n",
    "    metric=\"cosine\",\n",
    "    spec={\"serverless\": {\"cloud\": \"aws\", \"region\": \"us-east-1\"}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc712d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load API keys\n",
    "# -------------------------\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv(\"API_KEY_GEMINI\")\n",
    "pinecone_key = os.getenv(\"API_KEY_PINECONE\")\n",
    "\n",
    "if not gemini_key or not pinecone_key:\n",
    "    raise ValueError(\"‚ùå Missing API keys. Check your .env file.\")\n",
    "\n",
    "genai.configure(api_key=gemini_key)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Init Pinecone\n",
    "# -------------------------\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "index_name = \"acadmate-gemini\"   # ‚úÖ your new Gemini-compatible index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Load chunks from folder\n",
    "# -------------------------\n",
    "chunk_folder = r\"E:\\Plant_ML\\AI\\pdf_chunks\"\n",
    "files = sorted(os.listdir(chunk_folder))\n",
    "\n",
    "batch_size = 50\n",
    "to_upsert = []\n",
    "\n",
    "for i, filename in enumerate(files):\n",
    "    with open(os.path.join(chunk_folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        chunk = f.read()\n",
    "\n",
    "    # Gemini embedding (768-dim)\n",
    "    embedding_response = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=chunk\n",
    "    )\n",
    "    vector = embedding_response[\"embedding\"]\n",
    "\n",
    "    to_upsert.append({\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"values\": vector,\n",
    "        \"metadata\": {\"chunk_text\": chunk, \"chunk_file\": filename}\n",
    "    })\n",
    "\n",
    "    # Batch upsert\n",
    "    if len(to_upsert) >= batch_size:\n",
    "        index.upsert(to_upsert)\n",
    "        to_upsert = []\n",
    "\n",
    "if to_upsert:\n",
    "    index.upsert(to_upsert)\n",
    "\n",
    "print(f\"‚úÖ Stored {len(files)} chunks into Pinecone index '{index_name}'\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Query Pinecone\n",
    "# -------------------------\n",
    "query_text = \"Explain the Waterfall software process model\"\n",
    "\n",
    "query_embedding = genai.embed_content(\n",
    "    model=\"models/embedding-001\",\n",
    "    content=query_text\n",
    ")[\"embedding\"]\n",
    "\n",
    "results = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(\"\\nüîé Top Retrieved Chunks:\")\n",
    "for match in results.matches:\n",
    "    print(f\"Score: {match.score:.4f}\")\n",
    "    print(f\"File: {match.metadata['chunk_file']}\")\n",
    "    print(f\"Text: {match.metadata['chunk_text'][:300]}...\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. Generate final answer\n",
    "# -------------------------\n",
    "context = \"\\n\\n\".join([m.metadata[\"chunk_text\"] for m in results.matches])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\n",
    "    f\"Based on the following context, answer the question:\\n\\n{context}\\n\\nQuestion: {query_text}\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìù Final Answer:\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c6d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving context from Pinecone...\n",
      "Retrieved 6 chunks. Top scores: [0.834, 0.8217, 0.8076, 0.7925, 0.7883]\n",
      "\n",
      "Generating answer with Gemini (may take a few seconds)...\n",
      "\n",
      "\n",
      "=== MODEL OUTPUT ===\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "### Model Answer (student-facing)  **Introduction:**  The Waterfall model is a sequential software\n",
      "development process.  It proceeds linearly through distinct phases: communication, planning,\n",
      "modeling, construction, and deployment. This model is best suited for projects with stable, well-\n",
      "defined requirements where changes are minimal.  It's a classic approach, though its limitations in\n",
      "handling evolving requirements are well-documented.   **Process Stages:**  1. **Communication:**\n",
      "The initial phase focuses on understanding the project's objectives and gathering detailed\n",
      "requirements from stakeholders. This involves thorough discussions and documentation.  2.\n",
      "**Planning:** Based on the communication phase, a detailed project plan is created. This includes\n",
      "defining tasks, setting timelines, allocating resources, and establishing quality standards.  3.\n",
      "**Modeling:**  This stage involves creating models of the software system, including system design,\n",
      "architectural design, and component-level design.  These models provide a blueprint for the\n",
      "construction phase.  4. **Construction:**  The actual coding and implementation of the software\n",
      "system takes place during this phase.  This involves writing, testing, and debugging individual\n",
      "modules or components.  5. **Deployment:** Once the software is built and tested, it's deployed into\n",
      "the production environment. This may involve releasing the software to end users or integrating it\n",
      "with existing systems.  Post-deployment support and maintenance may also be considered as part of\n",
      "this phase, although the model's strict sequential nature can limit this.   **Advantages:**  *\n",
      "**Simplicity and ease of understanding:** The linear nature of the Waterfall model makes it\n",
      "straightforward to understand and manage, particularly for smaller projects.  Each phase has clear\n",
      "deliverables and milestones, simplifying progress tracking. * **Suitability for stable\n",
      "requirements:**  When requirements are well-defined and unlikely to change, the Waterfall model can\n",
      "be effective in delivering a consistent product that meets predefined specifications.\n",
      "**Disadvantages:**  * **Inflexibility:** The rigid sequential nature makes it difficult to\n",
      "accommodate changes in requirements once a phase is completed.  Changes often require restarting the\n",
      "entire process. * **Late detection of errors:**  Testing occurs primarily at the end of the\n",
      "construction phase, meaning that significant errors may be discovered very late in the development\n",
      "lifecycle, leading to costly and time-consuming rework.    ### Marking (teacher-facing)  - Intro: 2\n",
      "/ 2 ‚Äî  Clearly defines the Waterfall model and its applicability to projects with stable\n",
      "requirements. - Process Stages: 4 / 4 ‚Äî Accurately describes the five main phases (communication,\n",
      "planning, modeling, construction, deployment) with sufficient detail for each phase. - Advantages: 2\n",
      "/ 2 ‚Äî Correctly identifies simplicity and suitability for stable requirements as advantages. -\n",
      "Disadvantages: 2 / 2 ‚Äî Correctly points out inflexibility and late error detection as significant\n",
      "disadvantages. Total: 10 / 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'quit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 151\u001b[0m, in \u001b[0;36mrun_chatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     total_marks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal_marks_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'quit'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 195\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_fname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 195\u001b[0m     \u001b[43mrun_chatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 153\u001b[0m, in \u001b[0;36mrun_chatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m     total_marks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(total_marks_input)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     total_marks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal_marks_input\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    155\u001b[0m rubric_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRubric (format: criterion1:marks, criterion2:marks). Example: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntro:2, Theory:6, Example:2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnter rubric: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    156\u001b[0m rubric_items \u001b[38;5;241m=\u001b[39m parse_rubric_input(rubric_input)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'quit'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAG Chatbot for exam-style answers + automatic grading\n",
    "Requirements:\n",
    "  - .env set with API_KEY_GEMINI and API_KEY_PINECONE\n",
    "  - Pinecone index: 'acadmate-gemini' (768-dim embeddings)\n",
    "  - Python packages: google-generativeai, pinecone-client (or pinecone), python-dotenv, textwrap\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -------------------------\n",
    "# Config / init\n",
    "# -------------------------\n",
    "load_dotenv()\n",
    "GEMINI_KEY = os.getenv(\"API_KEY_GEMINI\")\n",
    "PINECONE_KEY = os.getenv(\"API_KEY_PINECONE\")\n",
    "INDEX_NAME = \"acadmate-gemini\"   # your Gemini-compatible index\n",
    "\n",
    "if not GEMINI_KEY or not PINECONE_KEY:\n",
    "    raise RuntimeError(\"Missing API keys in .env (API_KEY_GEMINI, API_KEY_PINECONE)\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def parse_rubric_input(rubric_str):\n",
    "    \"\"\"\n",
    "    Parse simple rubric text like:\n",
    "      \"introduction:2, theory:5, example:3\"\n",
    "    returns list of (criterion, marks)\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for part in rubric_str.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if \":\" in part:\n",
    "            crit, m = part.split(\":\", 1)\n",
    "            try:\n",
    "                items.append((crit.strip(), int(m.strip())))\n",
    "            except:\n",
    "                items.append((crit.strip(), float(m.strip())))\n",
    "        else:\n",
    "            # fallback: treat as unnamed criterion (whole marks)\n",
    "            items.append((part, 0))\n",
    "    return items\n",
    "\n",
    "def embed_query(text):\n",
    "    resp = genai.embed_content(model=\"models/embedding-001\", content=text)\n",
    "    return resp[\"embedding\"]\n",
    "\n",
    "def retrieve_context(query_text, top_k=6):\n",
    "    emb = embed_query(query_text)\n",
    "    results = index.query(vector=emb, top_k=top_k, include_metadata=True)\n",
    "    # Return list of chunk texts with scores\n",
    "    retrieved = []\n",
    "    for match in results.matches:\n",
    "        chunk = match.metadata.get(\"chunk_text\", \"\")\n",
    "        retrieved.append({\"score\": match.score, \"text\": chunk, \"file\": match.metadata.get(\"chunk_file\")})\n",
    "    return retrieved\n",
    "\n",
    "def build_system_prompt(rubric_items, total_marks, guidance=None):\n",
    "    \"\"\"\n",
    "    Build a system-level instruction for the model describing the evaluation scheme.\n",
    "    \"\"\"\n",
    "    rubric_lines = []\n",
    "    for crit, m in rubric_items:\n",
    "        rubric_lines.append(f\"- {crit} : {m} marks\")\n",
    "    rubric_text = \"\\n\".join(rubric_lines)\n",
    "    guidance_text = guidance.strip() if guidance else \"Answer precisely and concisely. Use clear headings and allocate content according to marks.\"\n",
    "    sys = (\n",
    "        \"You are an expert university-level examiner and tutor. \"\n",
    "        \"Given a question and a marks-scheme (rubric), produce a model answer aimed at a student \"\n",
    "        \"that exactly follows the rubric: include headings for each rubric criterion, allocate effort proportional \"\n",
    "        \"to the marks, and use numbered/bullet points if helpful. \"\n",
    "        \"After the model answer, provide a concise marking sheet that assigns marks per criterion and a short justification \"\n",
    "        \"for each awarded mark (self-grade). Do NOT invent facts beyond the provided context; prefer content from the context. \"\n",
    "        f\"Total marks: {total_marks}\\n\\nRubric:\\n{rubric_text}\\n\\nGuidance for style: {guidance_text}\\n\\n\"\n",
    "        \"Output format:\\n\"\n",
    "        \"### Model Answer (student-facing)\\n\"\n",
    "        \"<content>\\n\\n\"\n",
    "        \"### Marking (teacher-facing)\\n\"\n",
    "        \"- criterion: awarded_marks / max_marks ‚Äî justification (1-2 sentences)\\n\"\n",
    "        \"Total: X / TOTAL\\n\"\n",
    "    )\n",
    "    return sys\n",
    "\n",
    "def compose_prompt(context_chunks, question, system_instructions):\n",
    "    \"\"\"\n",
    "    Compose final prompt. We give the context (retrieved chunks), the question, and the instructions.\n",
    "    \"\"\"\n",
    "    # Take top N chunks' text concatenated, but keep them short - mark origins\n",
    "    ctx_parts = []\n",
    "    for i, c in enumerate(context_chunks):\n",
    "        ctx_parts.append(f\"[Chunk {i+1} | score={c['score']:.4f}]\\n{c['text']}\\n\")\n",
    "    context = \"\\n\\n\".join(ctx_parts)\n",
    "    prompt = (\n",
    "        f\"{system_instructions}\\n\\n\"\n",
    "        f\"CONTEXT (from the textbook / notes):\\n{context}\\n\\n\"\n",
    "        f\"QUESTION: {question}\\n\\n\"\n",
    "        \"Generate the Model Answer and the Marking as specified above.\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def call_gemini_generate(prompt, model_name=\"gemini-1.5-flash\"):\n",
    "    gm = genai.GenerativeModel(model_name)\n",
    "    # Use generate_content with text input\n",
    "    response = gm.generate_content(prompt)\n",
    "    # The SDK returns an object. We try to extract text safely:\n",
    "    try:\n",
    "        return response.text\n",
    "    except AttributeError:\n",
    "        # fallback - examine fields (some SDK versions return different fields)\n",
    "        return getattr(response, \"output_text\", str(response))\n",
    "\n",
    "def pretty_print_long(text):\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    for line in textwrap.wrap(text, width=100):\n",
    "        print(line)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# Interactive flow\n",
    "# -------------------------\n",
    "def run_chatbot():\n",
    "    print(\"RAG Exam Chatbot ‚Äî generate answers tailored to a rubric + self-grade\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        # Get question (direct entry or file path)\n",
    "        q_input = input(\"Enter question text (or path to .txt file): \").strip()\n",
    "        if q_input.lower() in (\"exit\", \"quit\", \"q\"):\n",
    "            break\n",
    "\n",
    "        if os.path.isfile(q_input):\n",
    "            with open(q_input, \"r\", encoding=\"utf-8\") as fh:\n",
    "                question = fh.read().strip()\n",
    "        else:\n",
    "            question = q_input\n",
    "\n",
    "        # Marks and rubric\n",
    "        total_marks_input = input(\"Total marks (e.g., 10): \").strip()\n",
    "        try:\n",
    "            total_marks = int(total_marks_input)\n",
    "        except:\n",
    "            total_marks = int(float(total_marks_input))\n",
    "\n",
    "        rubric_input = input(\"Rubric (format: criterion1:marks, criterion2:marks). Example: 'Intro:2, Theory:6, Example:2'\\nEnter rubric: \").strip()\n",
    "        rubric_items = parse_rubric_input(rubric_input)\n",
    "        # If rubric sums to 0 or sums != total_marks, normalize or warn\n",
    "        sum_marks = sum([m for _, m in rubric_items])\n",
    "        if sum_marks == 0:\n",
    "            # default single criterion equal to total\n",
    "            rubric_items = [(\"Answer\", total_marks)]\n",
    "            sum_marks = total_marks\n",
    "        if sum_marks != total_marks:\n",
    "            print(f\"[!] Rubric total {sum_marks} != Total marks {total_marks}. I'll keep rubric marks as provided and set Total={sum_marks}.\")\n",
    "            total_marks = sum_marks\n",
    "\n",
    "        # Optional guidance\n",
    "        guidance = input(\"Any special guidance for the answer (tone/length/keywords)? (press Enter to skip): \").strip()\n",
    "\n",
    "        # Retrieve context\n",
    "        print(\"\\nRetrieving context from Pinecone...\")\n",
    "        retrieved = retrieve_context(question, top_k=6)\n",
    "        print(f\"Retrieved {len(retrieved)} chunks. Top scores: {[round(r['score'], 4) for r in retrieved[:5]]}\")\n",
    "\n",
    "        # Build system instructions & prompt\n",
    "        sys_inst = build_system_prompt(rubric_items, total_marks, guidance=guidance)\n",
    "        prompt = compose_prompt(retrieved, question, sys_inst)\n",
    "\n",
    "        print(\"\\nGenerating answer with Gemini (may take a few seconds)...\")\n",
    "        result_text = call_gemini_generate(prompt)\n",
    "\n",
    "        # Print result\n",
    "        print(\"\\n\\n=== MODEL OUTPUT ===\")\n",
    "        pretty_print_long(result_text)\n",
    "\n",
    "        # Optionally save to file\n",
    "        save_opt = input(\"Save this output to file? (y/n): \").strip().lower()\n",
    "        if save_opt == \"y\":\n",
    "            safe_fname = \"rag_answer_output.txt\"\n",
    "            with open(safe_fname, \"w\", encoding=\"utf-8\") as fh:\n",
    "                fh.write(result_text)\n",
    "            print(f\"Saved to {safe_fname}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
